
# MapReduce

## Map
- Reads blocks/data and generate (key,value) pairs

## Reduce
- Reads (key,value) data generated by map phase and aggregates the result. Final result will be also in (key,value) format

## Map Reduce Execution Flow

<img src="https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2019/11/mapreduce-job-execution-flow.jpg">

## Input Data
- It is input files or refernce to the data stored in HDFS

## Input Format
- Defines how the input files are read from source or how to Serialize or Deserialize them properly

## Input Splits
- Logical representation of the data which will be processed by an individual Mapper task
- One Map task is created for each Input Split
- Data Block vs Input Split
  - ***Block:*** Actual unit where raw data will be stored physically
  - ***Input Splits:*** It is like a reference to the original block of data

## Record Reader
- It communicates with the Input Splits & converts the data into key-value pairs
- RecordReader by default uses TextInputFormat to convert data into a key-value pair
- For e.g. It will convert memory address as a key & data inside that address as a value

## Mapper
- It processes each input record from Record Reader and generates new intermediate key-value pair which is different from input key-value pair
- Hadoop framework doesnâ€™t store the output of mapper on HDFS(The data remains in memory only i.e in RAM).
- It takes the responsibility of heavy processing
- In this phase, we specify all the complex logic/business rules/costly code.

## Combiner
- It is like a mini-reducer
- It will aggregate results locally generated by each Mapper task

## Partitioner
- Partitioner comes into the existence if we are working with more than one reducer
- On the basis of key value in MapReduce, partitioning of each combiner output takes place using Hash function
- The record having the same key value goes into the same partition
- It allows even distribution of the Map output over the Reducer

## Shuffling & Sorting
- It will arrange the data for each key on individual reducer
- The shuffling is the physical movement of the data which is done over the network

## Reducer
- It will receiver Mapper tasks data as input and will generate resultant output
- The output of the reducer is the final output. Then framework stores the output on HDFS

## Output Format
- It decides in which format we want to write data in HDFS

## Output Files
- Generated reference for output data
