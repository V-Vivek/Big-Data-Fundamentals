# Data Integration
- Data integration is the process of combining data from multiple sources into a single, unified view. 
- The process of data integration includes a number of different steps, including
  ## Data Extraction
    - To extract data from one or more sources. This can include databases, flat files, or other systems.
    - The extracted data is usually stored in a staging area for further processing

  ## Data Transformation
    - It involves cleaning, validating, and transforming the data so that it is ready for loading into the destination system.
    - The transformation step can include tasks such as removing duplicates, converting data types, or calculating new values.

  ## Data Loading
    - To load the transformed data into the destination system. 
    - This can include a data warehouse, a data lake, or another system. 
    - The loaded data is then typically available for reporting, analysis, and other business processes.

  ## Data Quality Management
    - This step involves monitoring, identifying and correcting errors and inconsistencies in data
    - To ensure that the data is accurate and reliable.
  
  ## Data Governance
    - To ensure that the data is properly managed and protected throughout the data integration process
    - And ensures that data is being used in compliance with regulatory and legal requirements.

# Extract Transform Load (ETL)
<img src='https://panoply.io/uploads/versions/diagram4---x----750-328x---.jpg'>

- ETL process is focused on cleaning, transforming, and standardizing data before it is loaded into the target system.  
- This approach ensures data quality and consistency, but it can be time-consuming and require significant resources.

  ## Pros

  - ***Data quality***: ETL processes typically include data cleaning and validation steps, which can improve the quality of the data being loaded into the target system.
  - ***Data standardization***: ETL can be used to standardize data across different sources, making it easier to analyze and report on.
  - ***Automation***: ETL processes can be automated, which can save time and reduce errors.
  - ***Scalability***: ETL tools can handle large volumes of data, and can be scaled up as the volume of data increases.

## Cons

- ***Complexity***: ETL processes can be complex and require specialized knowledge and expertise to design and maintain.
- ***Time-consuming***: Extracting, transforming, and loading large volumes of data can take a long time, and can slow down the overall performance of the target system.
- ***Data loss***: If the ETL process is not properly designed or executed, data can be lost or corrupted.
- ***Cost***: ETL tools and processes can be expensive, and require significant investments in hardware, software, and personnel

 
# Etract Load Transform (ELT)
<img src='https://miro.medium.com/max/640/1*UAbW9DFAtSOqnz9-Mt2RSA.webp'>

- ELT process loads raw data into the target system first and then applies transformations and validations.  
- This approach is quicker to implement, and more cost-effective, but it may require more resources for data quality and integrity maintenance.

  ## Pros

  - Easy and Quicker to implement than ETL
  - Cheaper in comparison with ETL as Data Warehouse / Data Lake resources can be scaled using commodity hardware
  - Performance efficient , Scalable implementation hence less time to execute jobs, time to market
  - Raw data made available to DW layer, which can be used for analytical implementation

  ## Cons

  - Maintenance / Operational overhead can be little higher if various open-source tools used to implement ELT for Data Lake implementations
  - Need to implement appropriate cleaning, archival or roll ups on Data Warehouse / Data Lake system to avoid unwanted data to be preserved/stored to reduce system cost and improve performance
  - Data integrity and quality may suffer due to the lack of data validation or data transformation in the early stage of ELT process.

# Types of DataBase Management System (DBMS)
- 
    ## OLTP (Online Transaction Processing)
    - Designed to handle a large number of short transactions, such as inserting, updating, and deleting data. 
    - Typically used in operational environments, such as retail stores, banks and e-commerce websites. 
    - Optimized for fast data retrieval and data manipulation, and they typically use a ***normalized data model**** to minimize data redundancy.

-
  ## OLAP (Online Analytical Processing)
    - Designed for analytical and reporting purposes.
    - Used to analyze large amounts of historical data and provide insights into the data. 
    - Optimized for fast query performance and are typically built on top of a ***data warehouse or data mart***. 
    - Uses a ***denormalized data model***, which allows for faster query performance at the cost of increased data redundancy.

- In short, OLTP systems are focused on managing and processing transactions, while OLAP systems are focused on providing fast and flexible access to data for analytical and reporting purposes. 
- The two systems are often used in conjunction to support business operations, with OLTP systems capturing and managing transactional data and OLAP systems providing analytical capabilities on that data.




